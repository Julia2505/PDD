{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train Siamese Network (02.12.2019).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julia2505/PDD/blob/master/Train_Siamese_Network_(02_12_2019).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F2DbOsuRJ3t",
        "colab_type": "text"
      },
      "source": [
        "# Siamese Network for PDD data\n",
        "\n",
        "In this example we will show, how to train your own classifier using [Plant Disease Database](http://pdd.jinr.ru/crops.php)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEQsOtGHSBmO",
        "colab_type": "text"
      },
      "source": [
        "### Cloning the repo\n",
        "\n",
        "At first we will clone the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKEiGYnzc5fH",
        "colab_type": "code",
        "outputId": "89c717e2-8fa0-4e96-d440-a0487f27e588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!rm -r -f pdd_new\n",
        "!git clone https://github.com/AlexanderUzhinskiy/pdd_new.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pdd_new'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 278 (delta 0), reused 0 (delta 0), pack-reused 273\u001b[K\n",
            "Receiving objects: 100% (278/278), 53.90 MiB | 31.65 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98kZWpCZST5Z",
        "colab_type": "text"
      },
      "source": [
        "Change the directory to pdd_new to get access of helper functions and classes. Check github for mode details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qko1eieXbXta",
        "colab_type": "code",
        "outputId": "93f4d8af-6978-4563-831d-352c8c2cc13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.chdir('pdd_new')\n",
        "# verify if we are in correct directory\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README.md', '.gitignore', 'pdd', '.git', 'server', 'examples']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JZzbdilTFMp",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the grape pdd dataset \n",
        "\n",
        "We load it and split on train and test subdirectories in place. Chenge path and origin to your database. For sample of the structure download - http://pdd.jinr.ru/crops_nn.tar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b89c39ae-bfb6-4912-8123-dcd1bbd6ff67",
        "id": "RNfx5IjQ97vZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "from pdd.datasets.grape import load_data\n",
        "\n",
        "train_data_path, test_data_path = load_data(path='crops_nn.tar', origin=\"http://pdd.jinr.ru/crops_nn.tar\", split_on_train_test=True, random_state=13)\n",
        "print(train_data_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://pdd.jinr.ru/crops_nn.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|███████████| 35498/35498 [00:03<00:00, 10576.09it/s]\n",
            "100%|███████████████████| 15/15 [00:00<00:00, 121.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Splitting on train and test...\n",
            "/tmp/.pdd/datasets/crops_nn/train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYGPbc_DVxYS",
        "colab_type": "text"
      },
      "source": [
        "The structure of the dataset catalogue now:\n",
        "\n",
        "```\n",
        "grape\n",
        "│    \n",
        "└───train\n",
        "│   └───crop_disease_name\n",
        "│   |   │   20.jpg\n",
        "│   |   │   ...\n",
        "|   |\n",
        "|   └───crop_disease_name\n",
        "|   └───crop_disease_name\n",
        "|   └───crop_disease_name\n",
        "|   └───crop_disease_name\n",
        "|\n",
        "└───test\n",
        "    └───crop_disease_name\n",
        "    |   │   3.jpg\n",
        "    |   │   ...\n",
        "    |\n",
        "    └───crop_disease_name\n",
        "    └───crop_disease_name\n",
        "    └───crop_disease_name\n",
        "    └───crop_disease_name\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SnU7VEaVzlD",
        "colab_type": "text"
      },
      "source": [
        "### Create a feature extractor `twin` and a siamese network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg4HAly4duNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pdd.models import get_feature_extractor\n",
        "from pdd.models import make_siamese\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "# set the single session for tensorflow and keras both\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBkgrrT1W_Gh",
        "colab_type": "text"
      },
      "source": [
        "We are using the cross-entropy loss as in [this paper](http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) instead of contrastive loss. \n",
        "\n",
        "But feel free for using it by changing parameter `loss` to 'contrastive'. E.g.\n",
        "\n",
        "```python\n",
        "siams = make_siamese(feature_extractor, dist='l2', loss='contrastive')\n",
        "```\n",
        "\n",
        "There are three types of distances:\n",
        "\n",
        "\n",
        "*   `'l1'`\n",
        "*   `'l2'`\n",
        "*   `'cosine'`\n",
        "\n",
        "**But only `'l1'` is available for cross-entropy loss.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dlZXUPReNMA",
        "colab_type": "code",
        "outputId": "bfc5a378-2fb3-4388-e32d-bb8f0227597a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "source": [
        "input_shape = (256, 256, 3)\n",
        "\n",
        "print(\"Building feature extractor...\")\n",
        "feature_extractor = get_feature_extractor(input_shape)\n",
        "\n",
        "print(\"Constructing siamese network...\")\n",
        "siams = make_siamese(feature_extractor, dist='l1', loss='cross_entropy')\n",
        "siams.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building feature extractor...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Constructing siamese network...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 (None, 1024)         14902496    input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1024)         0           model_1[1][0]                    \n",
            "                                                                 model_1[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            1025        lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 14,903,521\n",
            "Trainable params: 14,901,537\n",
            "Non-trainable params: 1,984\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2djD033wYVAe",
        "colab_type": "text"
      },
      "source": [
        "### Creating a batch generator\n",
        "\n",
        "To train a siamese network data should be passed to the input as **positive-negative pairs**. Positive pair of images means a pair consists of images from the same class. Negative pairs consist of images of different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjJ1yf3ppqgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pdd.utils.training import SiameseBatchGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_iYx0D9ZNlu",
        "colab_type": "text"
      },
      "source": [
        "For training we are using a strong augmentation including rotations, zooming, flips and channel shifts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sxDA67Ep01t",
        "colab_type": "code",
        "outputId": "ba241ff5-1cdf-4213-bb5e-4a28403a79ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_batch_gen = SiameseBatchGenerator.from_directory(dirname=train_data_path, augment=True)\n",
        "test_batch_gen = SiameseBatchGenerator.from_directory(dirname=test_data_path)\n",
        "\n",
        "print(train_batch_gen) \n",
        "\n",
        "def siams_generator(batch_gen, batch_size=None):\n",
        "    while True:\n",
        "        batch_xs, batch_ys = batch_gen.next_batch(batch_size)\n",
        "        yield [batch_xs[0], batch_xs[1]], batch_ys"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pdd.utils.training.SiameseBatchGenerator object at 0x7f4ae22c57f0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJW0ECMjZf67",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "Let's train the model. \n",
        "\n",
        "**Note**, despite the fact that we are training the siamese model, the feature extractor is also being trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-2WjRv1eK9E",
        "colab_type": "code",
        "outputId": "367397d7-fe65-425a-f570-24356770d94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "siams.fit_generator(\n",
        "    generator=siams_generator(train_batch_gen),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_data=siams_generator(test_batch_gen),\n",
        "    validation_steps=30,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 142s 1s/step - loss: 0.0920 - acc: 0.9725 - val_loss: 0.5044 - val_acc: 0.8042\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.0933 - acc: 0.9688 - val_loss: 0.6036 - val_acc: 0.7458\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 137s 1s/step - loss: 0.0863 - acc: 0.9712 - val_loss: 0.4961 - val_acc: 0.7812\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.0928 - acc: 0.9659 - val_loss: 0.4921 - val_acc: 0.8042\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.0948 - acc: 0.9684 - val_loss: 0.5197 - val_acc: 0.8177\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.0801 - acc: 0.9728 - val_loss: 0.4240 - val_acc: 0.8250\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0804 - acc: 0.9728 - val_loss: 0.4510 - val_acc: 0.8333\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0790 - acc: 0.9722 - val_loss: 0.6373 - val_acc: 0.7927\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.0773 - acc: 0.9747 - val_loss: 0.5089 - val_acc: 0.7833\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.0792 - acc: 0.9756 - val_loss: 0.5321 - val_acc: 0.8146\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0815 - acc: 0.9738 - val_loss: 0.6574 - val_acc: 0.7896\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0873 - acc: 0.9700 - val_loss: 0.4707 - val_acc: 0.8260\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 133s 1s/step - loss: 0.0872 - acc: 0.9694 - val_loss: 0.4869 - val_acc: 0.8427\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0699 - acc: 0.9766 - val_loss: 0.4934 - val_acc: 0.8187\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0886 - acc: 0.9678 - val_loss: 0.4468 - val_acc: 0.8146\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0820 - acc: 0.9722 - val_loss: 0.4233 - val_acc: 0.8146\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.0685 - acc: 0.9784 - val_loss: 0.3007 - val_acc: 0.8521\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0542 - acc: 0.9813 - val_loss: 0.6699 - val_acc: 0.7656\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0660 - acc: 0.9759 - val_loss: 0.4387 - val_acc: 0.8500\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0826 - acc: 0.9703 - val_loss: 0.5398 - val_acc: 0.8010\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0596 - acc: 0.9813 - val_loss: 0.6480 - val_acc: 0.8063\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0809 - acc: 0.9753 - val_loss: 0.6151 - val_acc: 0.7958\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0783 - acc: 0.9747 - val_loss: 0.3837 - val_acc: 0.8417\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0694 - acc: 0.9741 - val_loss: 0.6028 - val_acc: 0.8167\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0655 - acc: 0.9781 - val_loss: 0.6081 - val_acc: 0.8156\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0536 - acc: 0.9828 - val_loss: 0.7917 - val_acc: 0.7594\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 134s 1s/step - loss: 0.0589 - acc: 0.9806 - val_loss: 0.6247 - val_acc: 0.7854\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 135s 1s/step - loss: 0.0889 - acc: 0.9650 - val_loss: 0.6666 - val_acc: 0.7740\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 136s 1s/step - loss: 0.0602 - acc: 0.9803 - val_loss: 0.4515 - val_acc: 0.8271\n",
            "Epoch 30/100\n",
            "  5/100 [>.............................] - ETA: 1:14 - loss: 0.0242 - acc: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d7471a85a22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msiams_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGUMr-KnamTW",
        "colab_type": "text"
      },
      "source": [
        "### K-nearest neighbors for classification\n",
        "\n",
        "Using of a siamese network for classification requires to perform an iterative n-shot task. But to avoid this restriction we can build a KNN classifier with the help of features extracted using the `twin` network.\n",
        "\n",
        "To significantly speed up the inference phase of the classifer we are going to utilize a fast k-nearest-neighbour search based on the method used in [\"Learning To Remember Rare Events\"](https://openreview.net/pdf?id=SJTQLdqlg)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbCgMWIY4tG1",
        "colab_type": "text"
      },
      "source": [
        "#### Save the feature extractor model and clear session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDT1R5wq44PF",
        "colab_type": "code",
        "outputId": "9cc1e4c3-3899-4995-f85f-7ccaa5eb3052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Saving feature extractor...\")\n",
        "feature_extractor.save('pdd_feature_extractor.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving feature extractor...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRFl_pk_4_EQ",
        "colab_type": "code",
        "outputId": "e2d95c4f-f531-405a-f1c4-4268a45cb80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "K.clear_session()\n",
        "tf.reset_default_graph()\n",
        "del sess"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxjxy5jW5Kj7",
        "colab_type": "text"
      },
      "source": [
        "#### Load the feature exctrator to KNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGpTRoY7mnoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import load_model\n",
        "\n",
        "from pdd.models import TfKNN\n",
        "from pdd.utils.data_utils import create_dataset_from_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBI1E8q05iEl",
        "colab_type": "code",
        "outputId": "2cffb943-38bc-4a28-d1bb-64808a42b88a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.Session()\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_session(sess)\n",
        "\n",
        "print(\"Loading feature extractor...\")\n",
        "feature_extractor = load_model(\"pdd_feature_extractor.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading feature extractor...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDvsUnvd5lsB",
        "colab_type": "text"
      },
      "source": [
        "#### Load datasets for the evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58HF_Dtx5qhe",
        "colab_type": "code",
        "outputId": "4b11b080-67e7-48d7-8385-be561c45b3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Loading datasets...\")\n",
        "train_dataset = create_dataset_from_dir(train_data_path, shuffle=True)\n",
        "test_dataset = create_dataset_from_dir(test_data_path, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading datasets...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cam8MkNP5vGJ",
        "colab_type": "text"
      },
      "source": [
        "#### Create KNN graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GwDgKQI5ysp",
        "colab_type": "code",
        "outputId": "fe6fb981-6fe3-4773-d4f8-4c82e37364b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "tfknn = TfKNN(sess, \n",
        "              feature_extractor, \n",
        "              (train_dataset['data'], train_dataset['target']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting keys from support set...\n",
            "--[_get_keys_from_support_set] took 1.10 seconds to run.\n",
            "\n",
            "Freezing feature extractor graph...\n",
            "WARNING:tensorflow:From /content/pdd_new/pdd/utils/graph_utils.py:30: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 52 variables.\n",
            "INFO:tensorflow:Converted 52 variables to const ops.\n",
            "WARNING:tensorflow:From /content/pdd_new/pdd/models/knn.py:32: The name tf.train.write_graph is deprecated. Please use tf.io.write_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/pdd_new/pdd/utils/graph_utils.py:37: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/pdd_new/pdd/utils/graph_utils.py:38: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
            "\n",
            "--[_freeze_feature_extractor_graph] took 0.42 seconds to run.\n",
            "\n",
            "Creating TfKNN graph...\n",
            "--[make_fknn_graph] took 0.02 seconds to run.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts1wL9AB53De",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41rTBE78nnRa",
        "colab_type": "code",
        "outputId": "5c81c2f4-0059-4d42-eee6-e7353a4b6edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# predictions and similarities\n",
        "preds, sims = tfknn.predict(test_dataset['data'])\n",
        "accuracy = accuracy_score(test_dataset['target'], preds)\n",
        "print(\"Accuracy: %.2f\" % accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making prediction for 70 images...\n",
            "--[predict] took 0.93 seconds to run.\n",
            "\n",
            "Accuracy: 0.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmGLtcsd5_sO",
        "colab_type": "text"
      },
      "source": [
        "#### Save the model for tensorflow serving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTUZ9Kik6DT5",
        "colab_type": "code",
        "outputId": "7be3da44-d703-4455-9618-752227854628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "tfknn.save_graph_for_serving(\"tfknn_graph\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving graph for serving...\n",
            "WARNING:tensorflow:From /content/pdd_new/pdd/models/knn.py:109: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: tfknn_graph/saved_model.pb\n",
            "--[save_graph_for_serving] took 0.20 seconds to run.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkTlwqZ26xSf",
        "colab_type": "text"
      },
      "source": [
        "*italicized text*## Upload model to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBgInXZzgcTf",
        "colab_type": "text"
      },
      "source": [
        "Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klc3pnQrgQLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxWxzVG6gq_I",
        "colab_type": "text"
      },
      "source": [
        "Copy files to gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IirXa6t1gxyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make directory in the destination\n",
        "!mkdir \"/gdrive/My Drive/pdd_model_0612\"\n",
        "\n",
        " #copy models .\n",
        "!cp tfknn_graph/saved_model.pb \"/gdrive/My Drive/pdd_model_0612/tf_graph.pb\"\n",
        "!cp pdd_feature_extractor.h5 \"/gdrive/My Drive/pdd_model_0612/pdd_feature_extractor.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}